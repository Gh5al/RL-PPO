{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PPO performance on Procgen\nIn this notebook we are going implement a Proximal Policy Optimization (PPO) agent. The experiment purpose is to evaluate the PPO agent generalization capabilities on a subset of environments provided by the Procgen benchmark.\\\nFor our experiment we will the following seeds and games:\\\nSeeds: 42,1377,47981\\\nGames: bigfish, dodgeball, fruitbot, maze, ninja, starpilot","metadata":{}},{"cell_type":"markdown","source":"# Useful links\n[https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details](url)\n[spinningup.openai.com/en/latest/algorithms/ppo.html](url)","metadata":{}},{"cell_type":"code","source":"#packages need to install to run procgen on the Gym API\n#!pip install procgen gym==0.26.2 pygame ufal.pybox2d","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nimport random\nimport pandas as pd\nimport pickle\nfrom collections import deque\nimport copy\nimport os,json \nimport gym\nfrom tqdm import tqdm\nfrom torch.distributions import Categorical","metadata":{"execution":{"iopub.status.busy":"2025-01-31T22:03:34.252412Z","iopub.execute_input":"2025-01-31T22:03:34.252778Z","iopub.status.idle":"2025-01-31T22:03:34.257277Z","shell.execute_reply.started":"2025-01-31T22:03:34.252752Z","shell.execute_reply":"2025-01-31T22:03:34.256392Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Reproducibility","metadata":{}},{"cell_type":"code","source":"#set the desired seed and the game from the Procgen benchmark\nSEED = 1377  \nGAME = 'starpilot'\n\n# Fix the random state for reproducibility\ndef fix_seed(seed: int) -> None:\n    \"\"\"Fix all the possible sources of randomness.\n\n    Args:\n        seed: the seed to use.\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n#For reproducibilty reasons we fix the seed \nfix_seed(SEED)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-31T22:03:35.394166Z","iopub.execute_input":"2025-01-31T22:03:35.394851Z","iopub.status.idle":"2025-01-31T22:03:35.420840Z","shell.execute_reply.started":"2025-01-31T22:03:35.394818Z","shell.execute_reply":"2025-01-31T22:03:35.420205Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Network Architecture","metadata":{}},{"cell_type":"markdown","source":"The backbone architecture of both the networks is inspired from the Impala paper: [https://arxiv.org/pdf/1802.01561](url) and also used in the Procgen paper: [https://arxiv.org/pdf/1912.01588](url)\n","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self,in_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.batch_norm = nn.BatchNorm2d(in_channels)\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        out = self.relu(x)\n        out = self.conv1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        return out + x\n    \nclass ImpalaBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ImpalaBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.res = ResidualBlock(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)\n        x = self.res(x)\n        x = self.res(x)\n        return x\n\nclass ActorImpalaModel(nn.Module):\n    def __init__(self,in_channels,output_size):\n        super(ActorImpalaModel, self).__init__()\n        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n        self.block4 = ImpalaBlock(in_channels=32, out_channels=64)\n        self.relu = nn.ReLU()\n        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=256)\n        self.out = nn.Linear(in_features=256,out_features=output_size)\n        self.conv = nn.Conv2d(in_channels=64, out_channels=output_size, kernel_size=1, stride=1)\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        #move the channels in the second dimension, from (n_batch,size1,size2,n_channels) to (n_batch,n_channels,size1,size2) \n        x = x.movedim(-1,1)\n        x = self.std(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.relu(x)\n        #flat the tensors from conv layers to fully connected layers\n        #1x1 convolution\n        x = self.conv(x)\n        #global average pooling\n        x = self.gap(x)\n        x = torch.flatten(x,1)\n        probs = self.softmax(x)\n        return probs\n    \n    #standardize the input\n    def std(self,x):\n        x = x / 255.0\n        return x    \n\nclass CriticImpalaModel(nn.Module):\n    def __init__(self,in_channels,output_size):\n        super(CriticImpalaModel, self).__init__()\n        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=128)\n        self.out = nn.Linear(in_features=128, out_features=output_size)\n        self.softmax = nn.Softmax(dim=1)\n        self.output_dim = 1\n\n    def forward(self, x):\n        x= x.movedim(-1,1)\n        x = self.std(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.relu(x)\n        #flat the tensors from conv layers to fully connected layers\n        x = torch.flatten(x,1)\n        x = self.fc(x)\n        x = self.tanh(x)\n        x = self.out(x)\n        return x\n    \n    def std(self,x):\n        x = x / 255.0\n        return x    \n\n\n\n\n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2025-01-31T22:20:46.106179Z","iopub.execute_input":"2025-01-31T22:20:46.106788Z","iopub.status.idle":"2025-01-31T22:20:46.121604Z","shell.execute_reply.started":"2025-01-31T22:20:46.106753Z","shell.execute_reply":"2025-01-31T22:20:46.120689Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"## Model summary","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport torch\nfrom torchsummary import summary\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nimpala = ActorImpalaModel(3,15).to(device)\nsummary(impala, (64,64,3))\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2025-01-31T22:03:57.352651Z","iopub.execute_input":"2025-01-31T22:03:57.352963Z","iopub.status.idle":"2025-01-31T22:03:57.359438Z","shell.execute_reply.started":"2025-01-31T22:03:57.352937Z","shell.execute_reply":"2025-01-31T22:03:57.358582Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"\"\\nimport torch\\nfrom torchsummary import summary\\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\\nimpala = ActorImpalaModel(3,15).to(device)\\nsummary(impala, (64,64,3))\\n\""},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# PPO implementation","metadata":{}},{"cell_type":"code","source":"class PPOAgent():\n    def __init__(self,\n                 in_ch,             #input channels\n                 n_actions,         #action space size\n                 n_envs = 48,       #n° of parallel environments\n                 batch_size = 256,  #size of the batch\n                 gamma = 0.99,      #PPO discount parameter\n                 lam = 0.95,        #GAE parameter\n                 epsilon = 0.2,     #PPO clipping parameter\n                 lr_a = 5e-4,       #actor learning rate\n                 lr_c = 5e-4,       #critic learning rate\n                 epochs = 3,        #n° of epochs in PPO \n                 n_minibatch = 6,   #number of minibatch\n                 device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),\n                 critic_criterion=torch.nn.MSELoss(reduction='mean')):\n        \n        self.in_ch = in_ch\n        self.n_actions = n_actions\n        self.n_envs = n_envs\n        self.batch_size = batch_size\n        self.gamma=gamma\n        self.lam = lam\n        self.epsilon = epsilon\n        self.lr_a = lr_a\n        self.lr_c = lr_c\n        self.epochs=epochs\n        self.n_minibatch = n_minibatch\n        self.device = device\n        self.critic_criterion = critic_criterion\n        self.actor,self.critic = self.get_networks()\n        self.actor_optimizer = torch.optim.AdamW(self.actor.parameters(), lr=self.lr_a, weight_decay=0.01)\n        self.critic_optimizer = torch.optim.AdamW(self.critic.parameters(), lr=self.lr_c, weight_decay=0.01)\n        \n     \n    def get_networks(self):\n        actor_net = ActorImpalaModel(self.in_ch,self.n_actions).to(self.device)\n        critic_net = CriticImpalaModel(self.in_ch,1).to(self.device)\n        return actor_net,critic_net\n    \n    def get_action(self,state):\n        #unsqueeze add a dimension of size 1 to simulate a batch\n        state = torch.tensor(state, dtype=torch.float32, device = self.device).unsqueeze(0)\n        #get probability distributions of the actions\n        probabilities = self.actor(state)\n        #build a distribution\n        dist = Categorical(probabilities.squeeze())\n        #sample action from the distribution\n        action = dist.sample()\n        #print(f\"multinom: {int(action.squeeze().detach().numpy())}\")\n        prob = probabilities.squeeze()[action]\n        return prob, action\n    \n    def play_step(self,obs):#obs has shape n_env x (64,64,3)\n        probs = self.actor(obs)\n        #compute log_prob and actions\n        dist = Categorical(probs)\n        #sample from distribution\n        actions = dist.sample()\n        #print(f\"multinom: {int(action.squeeze().detach().numpy())}\")\n        log_probs = dist.log_prob(actions).detach()\n        next_states,rewards,truncated, dones,infos=env.step(actions.cpu().detach().numpy())   \n        actions = actions.detach()\n        #dones = [d or t for d,t in zip(dones,truncated)]\n        dones = np.maximum(dones,truncated)\n        return next_states,rewards,dones,log_probs,actions\n    \n    #compute advantages and returns using GAE\n    def compute_advs_rts(self,values,rewards,dones,next_values):\n        A_t = 0\n        advantages = []\n        returns = []\n        values = torch.cat([values,next_values.unsqueeze(0)])\n        with torch.no_grad():\n            for t in reversed(range(len(rewards))):\n                delta = rewards[t] + self.gamma*values[t+1]*(~dones[t]) - values[t]\n                A_t =  delta + self.gamma * self.lam * A_t * (~dones[t])\n                advantages.insert(0,A_t.to(torch.float32))\n                ret = A_t + values[t]\n                returns.insert(0,ret.to(torch.float32))\n        return advantages, returns   \n        \n    def train(self,states,advantages,critic_targets,old_log_probs,actions):\n        actor_losses = []\n        critic_losses = []\n        #advantages normalization\n        advantages = (advantages - advantages.mean()) / (advantages.std()+1e-8)\n        #iterate for n epochs over the data, shuffle the data and create minibatches \n        tot_samples = self.n_envs*self.batch_size\n        for _ in range(self.epochs):\n            minibatch_size = tot_samples // self.n_minibatch\n            starts = np.arange(0,tot_samples,minibatch_size)\n            tot_ids = np.arange(0,tot_samples)\n            #randomly shuffle the batch\n            np.random.shuffle(tot_ids)\n            for start in starts:\n                ids = tot_ids[start:start+minibatch_size]    \n                #compute the values according to the updated critic\n                new_values = self.critic(states[ids])\n                \n                #compute the probabilities according to the updated actor\n                probs = self.actor(states[ids])\n                #make a distribution\n                dist = Categorical(probs)\n                #choose the new log probabilities\n                new_log_probs = dist.log_prob(actions[ids]).unsqueeze(1)\n                #compute the entropy, used to improve exploration\n                entropy = dist.entropy().unsqueeze(1)\n                \n                ratios = torch.exp(new_log_probs-old_log_probs[ids])\n                clip = torch.clamp(ratios, 1-self.epsilon, 1+self.epsilon)\n\n                entropy_loss = (0.01*torch.unsqueeze(entropy,1)).mean(0)\n                actor_loss =(-torch.min(ratios*advantages[ids], clip*advantages[ids])).mean(0)\n                actor_loss = actor_loss - entropy_loss\n                critic_loss = self.critic_criterion(new_values,critic_targets[ids])\n                #update actor\n                self.actor_optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor_optimizer.step()\n                #update critic\n                self.critic_optimizer.zero_grad()\n                critic_loss.backward()\n                self.critic_optimizer.step()\n            \n                actor_losses.append(actor_loss.item())\n                critic_losses.append(critic_loss.item())\n        return np.mean(actor_losses),np.mean(critic_losses)\n\n\n    #save and load models checkpoints\n    def save_checkpoint(self,step,game=GAME,seed=SEED):\n        filename1 = 'actor_checkpoint' \n        filename2 = 'critic_checkpoint'\n        models_folder = \"models_checkpoints\"\n        if not os.path.exists(models_folder):\n            os.makedirs(models_folder)\n        #actor saving\n        path = f\"{models_folder}/{game}_{filename1}_{step}_{seed}.pt\"\n        torch.save(self.actor.state_dict(),path)\n        #critic saving\n        path = f\"{models_folder}/{game}_{filename2}_{step}_{seed}.pt\"\n        torch.save(self.critic.state_dict(),path)\n        print(f\"Checkpoint:{step} with seed:{seed} created!\")\n\n    def load_checkpoint(self,step,game=GAME,seed=SEED):\n        if step%250 != 0 or step==0:\n            raise Exception(\"The step should be a multiple of 250 and greater than zero\")\n        models_folder = \"models_checkpoints\"\n        filename1 = 'actor_checkpoint' \n        filename2 = 'critic_checkpoint'\n        #actor loading\n        path = f\"{models_folder}/{game}_{filename1}_{step}_{seed}.pt\"\n        actor_dict = torch.load(path)\n        self.actor.load_state_dict(actor_dict)\n        #critic loading\n        path = f\"{models_folder}/{game}_{filename2}_{step}_{seed}.pt\"\n        critic_dict = torch.load(path)\n        self.critic.load_state_dict(critic_dict)\n        print(f\"Checkpoint:{step} have been loaded\")\n\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2025-01-31T22:04:00.525773Z","iopub.execute_input":"2025-01-31T22:04:00.526089Z","iopub.status.idle":"2025-01-31T22:04:00.607233Z","shell.execute_reply.started":"2025-01-31T22:04:00.526063Z","shell.execute_reply":"2025-01-31T22:04:00.606316Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"markdown","source":"## Save and load models","metadata":{}},{"cell_type":"code","source":"def save_model_weights(model_dict,seed=SEED,game=GAME,best=True):\n    if best:\n        filename1 = 'best_actor' \n        models_folder = \"best_models\"\n        if not os.path.exists(models_folder):\n            os.makedirs(models_folder)\n        #actor saving\n        path = f\"{models_folder}/{game}_{filename1}_{seed}.pt\"\n        torch.save(model_dict,path)\n    else:\n        filename1 = 'model' \n        models_folder = \"models\"\n        if not os.path.exists(models_folder):\n            os.makedirs(models_folder)\n        #model saving\n        path = f\"{models_folder}/{game}_{filename1}_{seed}.pt\"\n        torch.save(model_dict,path)\n    b = \"best\" if best else \"\"\n    print(f\"{b} model weights saved\")\n        \ndef load_model_weights(seed=SEED,game=GAME,best=True):\n    if best:\n        filename1 = 'best_actor' \n        models_folder = \"best_models\"\n        if not os.path.exists(models_folder):\n            os.makedirs(models_folder)\n        #best actor loading\n        path = f\"{models_folder}/{filename1}_{seed}.pt\"\n        model_dict = torch.load(path)\n    else:\n        models_folder = \"models\"\n        filename1 = 'model' \n        #actor loading\n        path = f\"{models_folder}/{filename1}_{seed}.pt\"\n        model_dict = torch.load(path)\n    b = \"best\" if best else \"\"\n    print(f\"{b} model weights loaded\")\n  \n    return model_dict \n \n","metadata":{"execution":{"iopub.status.busy":"2025-01-31T22:04:27.577895Z","iopub.execute_input":"2025-01-31T22:04:27.578601Z","iopub.status.idle":"2025-01-31T22:04:27.585399Z","shell.execute_reply.started":"2025-01-31T22:04:27.578568Z","shell.execute_reply":"2025-01-31T22:04:27.584476Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Save and load data","metadata":{}},{"cell_type":"code","source":"def save_data(df,train=True,game=GAME,seed=SEED):\n    if train:\n        filename= 'train_scores'\n    else:\n        filename= 'test_scores'\n    results_folder = \"results\"\n    if not os.path.exists(results_folder):\n        os.makedirs(results_folder)\n    path = f\"{results_folder}/{game}_{filename}_{SEED}.csv\"\n    df.to_csv(path)\n    print(f\"file created in {path}\")\n          \ndef load_data(game,seed,train=True):\n    if train:\n        filename= 'train_scores'\n    else:\n        filename= 'test_scores'\n    results_folder = \"results\"\n    path = f\"{results_folder}/{game}_{filename}_{SEED}.csv\"\n    df = pd.read_csv(path)\n    print(f\"file csv read from {path}\")\n    return df","metadata":{"execution":{"iopub.status.busy":"2025-01-31T22:04:31.196438Z","iopub.execute_input":"2025-01-31T22:04:31.197374Z","iopub.status.idle":"2025-01-31T22:04:31.204824Z","shell.execute_reply.started":"2025-01-31T22:04:31.197327Z","shell.execute_reply":"2025-01-31T22:04:31.203844Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"#evaluation on the full distribution\ndef evaluate(agent,test_env,test_size=10):\n    scores = []\n    lengths = []\n    for i in range(test_size):\n        sum = 0\n        l = 0\n        done = False\n        truncated = False\n        state,_= test_env.reset()\n        #pass the model to evaluation mode, to deal with batch_norm and dropout layers\n        agent.actor.eval()\n        with torch.no_grad():\n            while not(done or truncated):\n                _, action = agent.get_action(state)\n                act = int(action.detach().cpu().numpy())\n                next_state, reward, done, truncated, info = test_env.step(act)\n                state = next_state\n                l = l + 1\n                sum = sum + reward\n            scores.append(sum)\n            lengths.append(l)\n    average_score = np.mean(scores)\n    average_length = np.mean(lengths)\n    #return back to training mode\n    agent.actor.train()\n    return average_score,average_length\n","metadata":{"execution":{"iopub.status.busy":"2025-01-31T22:04:33.442417Z","iopub.execute_input":"2025-01-31T22:04:33.442775Z","iopub.status.idle":"2025-01-31T22:04:33.449187Z","shell.execute_reply.started":"2025-01-31T22:04:33.442745Z","shell.execute_reply":"2025-01-31T22:04:33.448303Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Gym Environments","metadata":{}},{"cell_type":"code","source":"game = GAME\nn_envs = 48\ntest_size = 10\n#train on a fixed subset of levels in easy mode\nenv = gym.vector.make(f'procgen:procgen-{game}-v0',\n               #render='human', \n               num_levels = 200, \n               distribution_mode = 'easy', \n               #use_backgrounds=False,\n               apply_api_compatibility = True,\n               start_level = SEED,\n               rand_seed = SEED,\n               num_envs=n_envs\n               )\nprint(f\"observation space shape:{env.observation_space.shape}\")\nprint(f\"action space size: {env.action_space[0]}\")\n\nn_states = env.observation_space.shape[0]\nn_actions = np.array(env.action_space)[0]\n\n\n#evaluate the agent on the full distribution of levels\ntest_env = gym.make(f'procgen:procgen-{game}-v0',\n                   #render='human', \n                    distribution_mode = 'easy',\n                    #num_levels = 0,\n                    #use_backgrounds=False,\n                    apply_api_compatibility = True,\n                    start_level = SEED,\n                    rand_seed = SEED)\n#use this env to evaluate the agent on the training levels\ntrain_env = gym.make(f'procgen:procgen-{game}-v0',\n                   #render='human', \n                    distribution_mode = 'easy',\n                    num_levels = 200,\n                    #use_backgrounds=False,\n                    apply_api_compatibility = True,\n                    start_level = SEED,\n                    rand_seed = SEED)","metadata":{"execution":{"iopub.status.busy":"2025-01-31T22:21:17.251123Z","iopub.execute_input":"2025-01-31T22:21:17.252051Z","iopub.status.idle":"2025-01-31T22:21:18.882281Z","shell.execute_reply.started":"2025-01-31T22:21:17.252013Z","shell.execute_reply":"2025-01-31T22:21:18.880986Z"},"trusted":true},"outputs":[{"name":"stdout","text":"observation space shape:(48, 64, 64, 3)\naction space size: Discrete(15)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"# Execution","metadata":{}},{"cell_type":"code","source":"def exec(agent, \n         env, #training environment\n         train_env, #evaluation environment for train levels\n         test_env,#evaluation environment for test levels\n         iterations, #total iterations\n         batch_size, #PPO:batch_size \n         n_envs, #number of parallel environments\n         results, #dictionary to memorize all the information\n         device, \n         seed=SEED):\n\n    #memorize latest best model\n    best_model = {'model':agent.actor.state_dict(),\n                  'score':0,\n                  'iteration':0}\n    #save results at each checkpoint in pickle file\n    results_folder = \"results\"\n    filename = 'all_res'\n    if not os.path.exists(results_folder):\n        os.makedirs(results_folder)\n    path = f\"{results_folder}/{GAME}_{filename}_{SEED}.pkl\"\n\n    #every 100 eps terminated compute an average and save the mean over 100 eps, and the start collecting again \n    train_scores = []\n    train_steps = []\n    train_lengths = []\n    #buffers used to save scores of evaluation\n    test_scores = []\n    test_steps = []\n    test_lengths = []\n    ftr_scores = [] \n    ftr_lengths = []\n    #count the total steps\n    tot_steps = 0\n    terminated_ep_rewards = [] #contains all the rewards from all the environments of all the terminated episodes\n    #reset the environments\n    states,_ = env.reset()\n    states=torch.stack([torch.tensor(o,device=device) for o in states])\n    #buffer used to compute the average reward of the last 100 terminated episodes \n    temp_scores = []\n    temp_lengths = []\n    actor_loss = []\n    critic_loss = []\n    for it in tqdm(range(1,iterations+1)):\n        #linearly reduce the learning rates every iteration \n        scale = 1 - ((it-1)/1500)\n        # anneal the optimizer's learning rate\n        agent.actor_optimizer.param_groups[0][\"lr\"] = agent.lr_a * scale\n        agent.critic_optimizer.param_groups[0][\"lr\"] = agent.lr_c * scale\n        \n        #used these buffers to collect information from the environments\n        batch_states = []\n        batch_values = []\n        batch_rewards =  []\n        batch_dones = []\n        batch_actions = []\n        batch_log_probs = []\n        sum_rewards = np.zeros((n_envs,), dtype=float)\n        sum_lengths = np.zeros((n_envs,),dtype=int)\n        \n        for t in range(batch_size):\n            #play a single step in the environments\n            next_states, rewards,dones,log_probs,actions = agent.play_step(states)\n            #add the new rewards to the previous ones\n            sum_rewards = sum_rewards + rewards\n            sum_lengths = sum_lengths + np.ones((n_envs,),dtype=int)\n            for i in range(len(dones)):\n                if dones[i]:\n                    terminated_ep_rewards.append(sum_rewards[i])\n                    temp_scores.append(sum_rewards[i])\n                    temp_lengths.append(sum_lengths[i])\n            #compute an average score of the last 100 terminated episodes\n            if len(temp_scores)>=100:\n                train_scores.append(np.mean(temp_scores[-100:]))\n                #save also the total num of steps when average is computed \n                train_steps.append(tot_steps)\n                train_lengths.append(np.mean(temp_lengths[-100:]))\n                #reset the buffer for new collection \n                temp_scores = []\n                temp_lengths = []\n            #reset tot rewards for the terminated episodes\n            sum_rewards[dones] = 0\n            #reset tot length for the terminated episodes\n            sum_lengths[dones] = 0\n            #compute state values using the critic\n            values = agent.critic(states).squeeze().to(device).detach()\n            #update the batches \n            batch_values.append(values)\n            batch_rewards.append(rewards)\n            batch_states.append(states)\n            batch_dones.append(dones)\n            batch_actions.append(actions)\n            batch_log_probs.append(log_probs)\n            states = torch.stack([torch.tensor(o,device=device) for o in next_states])\n            #increase the tot_step\n            tot_steps += 1\n        #compute the next_values of the last states,to be used for the computation of the advantages\n        next_values = agent.critic(states).squeeze().to(device).detach()\n        batch_values = torch.stack(batch_values).to(device)\n        batch_dones = torch.stack([torch.tensor(d, device=device, dtype=torch.bool) for d in batch_dones]).to(device)\n        #batch_rewards = torch.stack(batch_rewards).to(device)\n        batch_rewards = torch.stack([torch.tensor(r, device=device) for r in batch_rewards]).to(device)\n        #compute GAE advantages and returns (will be used as critic targets) for each timestep\n        advs,returns = agent.compute_advs_rts(batch_values,batch_rewards,batch_dones,next_values)\n\n        #flatten the data collected from the different environments\n        batch_states = torch.stack(batch_states)\n        #reshape the batches by collapsing the batch and n_environment dimensions\n        obs_size = batch_states.size()[2:]\n        batch_states = batch_states.reshape((tuple([n_envs*batch_size])+obs_size))\n        advs = torch.stack(advs).reshape(-1,1).to(device)\n        returns = torch.stack(returns).reshape(-1,1).to(device)\n        batch_actions = torch.stack(batch_actions).reshape(-1).to(device).detach()\n        batch_log_probs = torch.stack(batch_log_probs).reshape((-1,1)).to(device).detach()\n\n        #learning phase\n        a_loss, c_loss = agent.train(batch_states,advs,returns,batch_log_probs,batch_actions)\n        #save losses\n        actor_loss.append(a_loss)\n        critic_loss.append(c_loss)\n        #print(f\"mean_reward: {np.mean(terminated_ep_rewards[-20:])}, n°of steps: {tot_steps}, n° of terminated episodes: {len(terminated_ep_rewards)}\")\n        #evaluation phase, after each iteration compute an evaluation score\n        if ((it%2) == 0):\n            test_score,test_l = evaluate(agent,test_env)\n            ftr_score,ftr_l = evaluate(agent,train_env)\n            ftr_scores.append(ftr_score)\n            ftr_lengths.append(ftr_l)\n            test_scores.append(test_score)\n            test_steps.append(tot_steps)\n            test_lengths.append(test_l)\n            if(best_model['score']<=(np.mean(test_scores[-2:]))):\n                #save the model parameters\n                best_model.update({'model':agent.actor.state_dict(),'score':test_score,'iteration':it})\n                \n            print(f\"iteration:{it},'tot_steps': {tot_steps}, test_score:{test_score}, train_score:{ftr_score}\")\n        if (it%250 == 0):\n            print(f\"iteration:{it}, tot_steps:{tot_steps}\")\n            #save checkpoint\n            agent.save_checkpoint(step=it)\n            #save best_model\n            save_model_weights(best_model['model'],seed=seed,best=True)\n        if (it%100 == 0):\n            #save results to pkl file\n            with open(path, 'wb') as f:\n                pickle.dump(final_res, f)\n        if (it%10 == 0):    \n            final_res = {'train_scores':train_scores,'train_steps':train_steps,'train_lengths':train_lengths,\n                         'test_scores':test_scores,'test_steps':test_steps,'test_lengths':test_lengths,\n                         'tot_eps_rws':terminated_ep_rewards,'best_model':best_model,'actor_loss':actor_loss,\n                         'critic_loss':critic_loss,'ftr_scores': ftr_scores,'ftr_lengths':ftr_lengths}\n\n            results.update(final_res)\n                            \n    return results","metadata":{"execution":{"iopub.status.busy":"2025-01-31T22:21:22.780577Z","iopub.execute_input":"2025-01-31T22:21:22.780971Z","iopub.status.idle":"2025-01-31T22:21:22.803177Z","shell.execute_reply.started":"2025-01-31T22:21:22.780931Z","shell.execute_reply":"2025-01-31T22:21:22.802265Z"},"trusted":true},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"# Run","metadata":{}},{"cell_type":"code","source":"#tot_timesteps = 1000 * 256 * 48 = 12 288 000 \n#parameters\nin_ch = 3\nn_actions = 15\nbatch_size = 256\niterations = 1000\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nagent = PPOAgent(in_ch,n_actions,n_envs=n_envs,batch_size=batch_size,device=device )","metadata":{"execution":{"iopub.status.busy":"2025-01-31T22:21:29.957118Z","iopub.execute_input":"2025-01-31T22:21:29.957916Z","iopub.status.idle":"2025-01-31T22:21:29.987643Z","shell.execute_reply.started":"2025-01-31T22:21:29.957882Z","shell.execute_reply":"2025-01-31T22:21:29.986984Z"},"trusted":true},"outputs":[],"execution_count":37},{"cell_type":"code","source":"results = {}\nfinal_results = exec(agent,\n                     env,\n                     train_env,\n                     test_env,\n                     iterations,\n                     batch_size,\n                     n_envs,\n                     results,\n                     device,\n                     SEED)","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null}]}