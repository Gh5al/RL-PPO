{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install procgen\n#!pip install gym==0.26.2 \n!pip install torchsummary\n#!pip install pygame ufal.pybox2d","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:23:40.256732Z","iopub.execute_input":"2024-10-14T18:23:40.257006Z","iopub.status.idle":"2024-10-14T18:24:09.683397Z","shell.execute_reply.started":"2024-10-14T18:23:40.256981Z","shell.execute_reply":"2024-10-14T18:24:09.682385Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nimport random\nimport pandas as pd\nimport pickle\nfrom collections import deque\nimport copy\nimport os,json \nimport gym\nfrom tqdm import tqdm\nfrom torch.distributions import Categorical","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:24:09.685717Z","iopub.execute_input":"2024-10-14T18:24:09.686609Z","iopub.status.idle":"2024-10-14T18:24:09.691803Z","shell.execute_reply.started":"2024-10-14T18:24:09.686569Z","shell.execute_reply":"2024-10-14T18:24:09.690929Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reproducibility","metadata":{}},{"cell_type":"markdown","source":"The following notebook will be executed over 3 seeds for each game.\\\nseeds: 42,1377,47981\\\ngames: coinrun,fruitbot,\none run with squeeze excitation module","metadata":{}},{"cell_type":"code","source":"# Fix the random state for reproducibility\nSEED = 42\nGAME = 'jumper'\n\ndef fix_seed(seed: int) -> None:\n    \"\"\"Fix all the possible sources of randomness.\n\n    Args:\n        seed: the seed to use.\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n#For reproducibilty reasons we fix the seed \nfix_seed(SEED)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:27:40.720569Z","iopub.execute_input":"2024-10-14T18:27:40.720927Z","iopub.status.idle":"2024-10-14T18:27:40.730646Z","shell.execute_reply.started":"2024-10-14T18:27:40.720901Z","shell.execute_reply":"2024-10-14T18:27:40.729813Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Network Architecture","metadata":{}},{"cell_type":"code","source":"#64-3+2 / 1 + 1 64\n#64-3+2 / 1 + 1 64\n#64\n#H and W reduction with maxpool\n#64-3+2 / 2 = 32\n#32-3+2 / 2 = 16\n#16-3+2 / 2 = 8 \n#modified impala, after each convolutional layer, add a batch normalization layer\nclass ResidualBlock(nn.Module):\n    def __init__(self,in_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n        self.batch_norm = nn.BatchNorm2d(in_channels)\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        out = self.relu(x)\n        out = self.conv1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        return out + x\n\nclass SqueezeExcitation(nn.Module):\n    def __init__(self,in_channels,ratio = 16):\n        super(SqueezeExcitation, self).__init__()\n        #ratio is the reduction ratio\n        #as we want to compute global average pooling, we can either use adaptive avg pooling followed by some squeezing ops,\n        #or use a mean op on HXW and the apply the fully connected layers\n        \n        self.avg_pool = nn.AdaptiveAvgPool2d(1) #output size is 1, as we want to work on channels\n        #squeeze the n° of channels \n        self.fc1 = nn.Linear(in_channels,in_channels//ratio)\n        self.relu = nn.ReLU()\n        #expand the n° of channels \n        self.fc2 = nn.Linear(in_channels//ratio,in_channels)\n        self.sigmoid= nn.Sigmoid()\n    \n    def forward(self,input):\n        #x = self.avg_pool(input)\n        #x = x.squeeze([2,3])\n        x = input.mean([2,3])\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        #restore the tensor size\n        x = x.unsqueeze(2).unsqueeze(3)\n        #scale\n        out = input*x\n        return out\n    \nclass ImpalaBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ImpalaBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.res = ResidualBlock(out_channels)\n        self.se =  SqueezeExcitation(out_channels)\n        #self.res2 = ResidualBlock(out_channels)\n\n    def forward(self, x):\n        x = self.batch_norm(self.conv(x))\n        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)\n        x = self.res(x)\n        x = self.se(x)\n        x = self.res(x)\n        x = self.se(x)\n        return x\n\nclass ActorImpalaModel(nn.Module):\n    def __init__(self,in_channels,output_size):\n        super(ActorImpalaModel, self).__init__()\n        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n        self.block4 = ImpalaBlock(in_channels=32, out_channels=64)\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=256)\n        self.out = nn.Linear(in_features=256,out_features=output_size)\n        self.conv = nn.Conv2d(in_channels=64, out_channels=output_size, kernel_size=1, stride=1)\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        #move the channels in the second dimension, from (n_batch,size1,size2,n_channels) to (n_batch,n_channels,size1,size2) \n        x = x.movedim(-1,1)\n        x = self.std(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.relu(x)\n        #flat the tensors from conv layers to fully connected layers\n        #todo: replace fullyconnected layer with global average pooling with output_size num of channels\n        #1x1 convolution\n        x = self.conv(x)\n        x = self.gap(x)\n        #x = self.tanh(x)\n\n        x = torch.flatten(x,1)\n        #x = self.tanh(self.fc(x))\n        #x = self.out(x)\n        probs = self.softmax(x)\n        return probs\n    #standardize the input\n    def std(self,x):\n        x = x / 255.0\n        return x    \n\nclass CriticImpalaModel(nn.Module):\n    def __init__(self,in_channels,output_size):\n        super(CriticImpalaModel, self).__init__()\n        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=128)\n        self.out = nn.Linear(128,1)\n        self.softmax = nn.Softmax(dim=1)\n        self.output_dim = 1\n\n    def forward(self, x):\n        x= x.movedim(-1,1)\n        x = self.std(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.relu(x)\n        #flat the tensors from conv layers to fully connected layers\n        x = torch.flatten(x,1)\n        x = self.fc(x)\n        x = self.tanh(x)\n        x = self.out(x)\n        return x\n    \n    def std(self,x):\n        x = x / 255.0\n        return x    \n\n\n\n\n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:27:41.931770Z","iopub.execute_input":"2024-10-14T18:27:41.932128Z","iopub.status.idle":"2024-10-14T18:27:41.959753Z","shell.execute_reply.started":"2024-10-14T18:27:41.932099Z","shell.execute_reply":"2024-10-14T18:27:41.958793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CriticNatureModel(nn.Module):\n#64-8+2 / 3 + 1 = 20\n#20 -4 +2 / 2 +1 = 10\n#15-3+2 + 1  = 15\n#84-8+2 / 4 +1 = 20.5\n#20 -4 + 2 /2  + 1 = 10 \n    def __init__(self,in_ch,output_size= 1):\n        super(CriticNatureModel, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_ch, out_channels=32, kernel_size=8, stride=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.fc = nn.Linear(10*10*64,256)\n        self.out = nn.Linear(256,output_size)\n        #self.softmax = nn.Softmax(dim=1)\n    \n    def forward(self,x):\n        x = x.movedim(-1,2)\n\n        #x = x.view(x.shape[0],-1,64,64)\n        x = x.reshape((x.size()[0],-1,64,64))\n        x = self.std(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.relu(self.conv3(x))\n        x = torch.flatten(x,1)\n        x = self.fc(x)\n        value = self.out(x)\n        return value\n    \n    def std(self,x):\n        x = x / 255.0\n        return x    \n\nclass ActorNatureModel(nn.Module):\n#64-8+2 / 3 + 1 = 20\n#20 -4 +2 / 2 +1 = 10\n#15-3+2 + 1  = 15\n#84-8+2 / 4 +1 = 20.5\n#20 -4 + 2 /2  + 1 = 10 \n    def __init__(self,in_ch,output_size):\n        super(ActorNatureModel, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_ch, out_channels=32, kernel_size=8, stride=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.fc = nn.Linear(10*10*64,512)\n        self.out = nn.Linear(512,output_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self,x,frame=True):\n        x = x.movedim(-1,1)\n        if frame:\n            x = x.reshape((x.size()[0],-1,64,64))\n        x = self.std(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.relu(self.conv3(x))\n        x = torch.flatten(x,1)\n        x = self.fc(x)\n        logits = self.out(x)\n        return self.softmax(logits)\n\n    def std(self,x):\n        x = x / 255.0\n        return x    \nclass CriticNatureModel(nn.Module):\n#64-8+2 / 3 + 1 = 20\n#20 -4 +2 / 2 +1 = 10\n#15-3+2 + 1  = 15\n#84-8+2 / 4 +1 = 20.5\n#20 -4 + 2 /2  + 1 = 10 \n    def __init__(self,in_ch,output_size= 1):\n        super(CriticNatureModel, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_ch, out_channels=32, kernel_size=5, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.maxpool = nn.MaxPool2d(2,2)\n        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        #self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.conv4 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding=1)\n        self.fc = nn.Linear(17*17*32,128)\n        self.out = nn.Linear(128,output_size)\n        #self.softmax = nn.Softmax(dim=1)\n    \n    def forward(self,input):\n        input = input.movedim(-1,1)\n        x = self.std(input)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.relu(self.maxpool(x))\n        x = self.conv3(x) \n        x = self.relu(self.conv4(x))\n        x = torch.flatten(x,1)\n        x = self.fc(x)\n        value = self.out(x)\n        return value\n    \n    def std(self,x):\n        x = x / 255.0\n        return x    \n\nclass ActorNatureModel(nn.Module):\n#64-5+2 / 2 + 1 = 31\n#32 -3 +2 / 1 +1 = 31\n#relu + max pooling\n\n#15-3+2 + 1  = 15\n#15-3+2+ 1 = 15\n#15-1+2 + 1 =17\n#84-8+2 / 4 +1 = 20.5\n#20 -4 + 2 /2  + 1 = 10 \n    def __init__(self,in_ch,output_size):\n        super(ActorNatureModel, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_ch, out_channels=32, kernel_size=5, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.maxpool = nn.MaxPool2d(2,2)\n        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        #self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.conv4 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding=1)\n        self.fc = nn.Linear(17*17*32,128)\n        self.out = nn.Linear(128,output_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self,input):\n        input = input.movedim(-1,1)\n        x = self.std(input)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.relu(self.maxpool(x))\n        x = self.conv3(x) \n        x = self.relu(self.conv4(x))\n        x = torch.flatten(x,1)\n        x = self.fc(x)\n        logits = self.out(x)\n        return self.softmax(logits)\n\n    def std(self,x):\n        x = x / 255.0\n        return x    \n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:27:42.199272Z","iopub.execute_input":"2024-10-14T18:27:42.199614Z","iopub.status.idle":"2024-10-14T18:27:42.227876Z","shell.execute_reply.started":"2024-10-14T18:27:42.199586Z","shell.execute_reply":"2024-10-14T18:27:42.226966Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model summary","metadata":{}},{"cell_type":"code","source":"import torch\n#from torchvision import models\nfrom torchsummary import summary\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\nmodel = ActorNatureModel(3,15).to(device)\ncritic = CriticNatureModel(3,1).to(device)\nimpala = ActorImpalaModel(3,15).to(device)\nsummary(impala, (64,64,3))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:27:42.855174Z","iopub.execute_input":"2024-10-14T18:27:42.855526Z","iopub.status.idle":"2024-10-14T18:27:44.209670Z","shell.execute_reply.started":"2024-10-14T18:27:42.855498Z","shell.execute_reply":"2024-10-14T18:27:44.208675Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PPO implementation","metadata":{}},{"cell_type":"code","source":"class PPOAgent():\n    def __init__(self,\n                 in_ch,\n                 n_actions,\n                 n_envs = 32,\n                 batch_size = 256,\n                 gamma = 0.99,\n                 lam = 0.95,\n                 epsilon = 0.2,\n                 lr_a = 5e-4,\n                 lr_c = 5e-4,\n                 epochs = 3,\n                 n_minibatch = 6,\n                 device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),\n                 critic_criterion=torch.nn.MSELoss(reduction='mean')):\n        \n        self.in_ch = in_ch\n        self.n_actions = n_actions\n        self.n_envs = n_envs\n        self.batch_size = batch_size\n        self.gamma=gamma\n        self.lam = lam\n        self.epsilon = epsilon\n        self.lr_a = lr_a\n        self.lr_c = lr_c\n        self.epochs=epochs\n        self.n_minibatch = n_minibatch\n        self.device = device\n        self.critic_criterion = critic_criterion\n        self.actor,self.critic = self.get_networks()\n        self.actor_optimizer = torch.optim.AdamW(self.actor.parameters(), lr=self.lr_a, weight_decay=0.01)\n        self.critic_optimizer = torch.optim.AdamW(self.critic.parameters(), lr=self.lr_c, weight_decay=0.01)\n        \n     \n    def get_networks(self):\n        actor_net = ActorImpalaModel(self.in_ch,self.n_actions).to(self.device)\n        critic_net = CriticImpalaModel(self.in_ch,1).to(self.device)\n        return actor_net,critic_net\n    \n    def get_action(self,state):\n        #unsqueeze add a dimension of size 1 to simulate a batch\n        state = torch.tensor(state, dtype=torch.float32, device = self.device).unsqueeze(0)\n        #get probability distributions of the actions\n        probabilities = self.actor(state)\n        #build a distribution\n        dist = Categorical(probabilities.squeeze())\n        #sample action from the distribution\n        action = dist.sample()\n        #print(f\"multinom: {int(action.squeeze().detach().numpy())}\")\n        prob = probabilities.squeeze()[action]\n        return prob, action\n    \n    def get_state_value(self,state):\n        #unsqueeze add a dimension of size 1 to simulate a batch\n        state = np.array(state)\n        state = torch.tensor(state, dtype=torch.float32, device = self.device).unsqueeze(0)\n        value = self.critic(state)\n        return value\n    \n    def play_step(self,obs):#obs has shape n_env x (64,64,3)\n        probs = self.actor(obs)\n        #compute log_prob and actions\n        dist = Categorical(probs)\n        #sample from distribution\n        actions = dist.sample()\n        #print(f\"multinom: {int(action.squeeze().detach().numpy())}\")\n        log_probs = dist.log_prob(actions).detach()\n        next_states,rewards,truncated, dones,infos=env.step(actions.cpu().detach().numpy())   \n        actions = actions.detach()\n        #dones = [d or t for d,t in zip(dones,truncated)]\n        dones = np.maximum(dones,truncated)\n        return next_states,rewards,dones,log_probs,actions\n    \n    #compute advantages and returns as GAE\n    def compute_advs_rts(self,values,rewards,dones,next_values):\n        A_t = 0\n        advantages = []\n        returns = []\n        values = torch.cat([values,next_values.unsqueeze(0)])\n        with torch.no_grad():\n            for t in reversed(range(len(rewards))):\n                delta = rewards[t] + self.gamma*values[t+1]*(~dones[t]) - values[t]\n                A_t =  delta + self.gamma * self.lam * A_t * (~dones[t])\n                advantages.insert(0,A_t.to(torch.float32))\n                ret = A_t + values[t]\n                returns.insert(0,ret.to(torch.float32))\n        return advantages, returns   \n    \n    def train(self,states,advantages,critic_targets,old_log_probs,actions):\n        actor_losses = []\n        critic_losses = []\n        #advantages normalization\n        advantages = (advantages - advantages.mean()) / (advantages.std()+1e-8)\n        #iterate for n epochs over the data, by shuffling the data and creating minibatches \n        tot_samples = self.n_envs*self.batch_size\n        \n        for _ in range(self.epochs):\n            minibatch_size = tot_samples // self.n_minibatch\n            starts = np.arange(0,tot_samples,minibatch_size)\n            tot_ids = np.arange(0,tot_samples)\n            np.random.shuffle(tot_ids)\n            for start in starts:\n                #randomly shuffle the batch\n                ids = tot_ids[start:start+minibatch_size]\n                \n                #compute the values according to the updated critic\n                new_values = self.critic(states[ids])\n                \n                #compute the probabilities according to the updated actor\n                probs = self.actor(states[ids])\n                #make a distribution\n                dist = Categorical(probs)\n                #choose the new log probabilities\n                new_log_probs = dist.log_prob(actions[ids]).unsqueeze(1)\n                #compute the entropy, used to improve exploration\n                entropy = dist.entropy().unsqueeze(1)\n                \n                ratios = torch.exp(new_log_probs-old_log_probs[ids])\n                clip = torch.clamp(ratios, 1-self.epsilon, 1+self.epsilon)\n                \n                \n                entropy_loss = (0.01*torch.unsqueeze(entropy,1)).mean(0)\n                actor_loss =(-torch.min(ratios*advantages[ids], clip*advantages[ids])).mean(0)\n                actor_loss = actor_loss - entropy_loss\n\n                critic_loss = self.critic_criterion(new_values,critic_targets[ids])\n                #update actor\n                self.actor_optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor_optimizer.step()\n                \n                #update critic\n                self.critic_optimizer.zero_grad()\n                critic_loss.backward()\n                self.critic_optimizer.step()\n            \n                actor_losses.append(actor_loss.item())\n                critic_losses.append(critic_loss.item())\n        return np.mean(actor_losses),np.mean(critic_losses)\n\n\n    #save and load models checkpoints\n    def save_checkpoint(self,step,game=GAME,seed=SEED):\n        filename1 = 'actor_checkpoint' \n        filename2 = 'critic_checkpoint'\n        models_folder = \"models_checkpoints\"\n        if not os.path.exists(models_folder):\n            os.makedirs(models_folder)\n        #actor saving\n        path = f\"{models_folder}/{game}_{filename1}_{step}_{seed}.pt\"\n        torch.save(self.actor.state_dict(),path)\n        #critic saving\n        path = f\"{models_folder}/{game}_{filename2}_{step}_{seed}.pt\"\n        torch.save(self.critic.state_dict(),path)\n        print(f\"Checkpoint:{step} with seed:{seed} created!\")\n\n    def load_checkpoint(self,step,game=GAME,seed=SEED):\n        if step%250 != 0 or step==0:\n            raise Exception(\"The step should be a multiple of 250 and greater than zero\")\n        models_folder = \"models_checkpoints\"\n        filename1 = 'actor_checkpoint' \n        filename2 = 'critic_checkpoint'\n        #actor loading\n        path = f\"{models_folder}/{game}_{filename1}_{step}_{seed}.pt\"\n        actor_dict = torch.load(path)\n        self.actor.load_state_dict(actor_dict)\n        #critic loading\n        path = f\"{models_folder}/{game}_{filename2}_{step}_{seed}.pt\"\n        critic_dict = torch.load(path)\n        self.critic.load_state_dict(critic_dict)\n        print(f\"Checkpoint:{step} have been loaded\")\n\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:27:44.211352Z","iopub.execute_input":"2024-10-14T18:27:44.211662Z","iopub.status.idle":"2024-10-14T18:27:44.243126Z","shell.execute_reply.started":"2024-10-14T18:27:44.211636Z","shell.execute_reply":"2024-10-14T18:27:44.242224Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"markdown","source":"## Save and load models","metadata":{}},{"cell_type":"code","source":"def save_model_weights(model_dict,seed=SEED,game=GAME,best=True):\n    if best:\n        filename1 = 'best_actor' \n        #filename2 = 'best_critic'\n        models_folder = \"best_models\"\n        if not os.path.exists(models_folder):\n            os.makedirs(models_folder)\n        #actor saving\n        path = f\"{models_folder}/{game}_{filename1}_{seed}.pt\"\n        torch.save(model_dict,path)\n    else:\n        filename1 = 'model' \n        models_folder = \"models\"\n        if not os.path.exists(models_folder):\n            os.makedirs(models_folder)\n        #model saving\n        path = f\"{models_folder}/{game}_{filename1}_{seed}.pt\"\n        torch.save(model_dict,path)\n    b = \"best\" if best else \"\"\n    print(f\"{b} model weights saved\")\n        \ndef load_model_weights(seed=SEED,game=GAME,best=True):\n    if best:\n        filename1 = 'best_actor' \n        models_folder = \"best_models\"\n        if not os.path.exists(models_folder):\n            os.makedirs(models_folder)\n        #best actor loadin\n        path = f\"{models_folder}/{filename1}_{seed}.pt\"\n        model_dict = torch.load(path)\n    else:\n        models_folder = \"models\"\n        filename1 = 'model' \n        #actor loading\n        path = f\"{models_folder}/{filename1}_{seed}.pt\"\n        model_dict = torch.load(path)\n    b = \"best\" if best else \"\"\n    print(f\"{b} model weights loaded\")\n  \n    return model_dict \n \n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:27:44.789972Z","iopub.execute_input":"2024-10-14T18:27:44.790524Z","iopub.status.idle":"2024-10-14T18:27:44.799305Z","shell.execute_reply.started":"2024-10-14T18:27:44.790491Z","shell.execute_reply":"2024-10-14T18:27:44.798499Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save and load data","metadata":{}},{"cell_type":"code","source":"def save_data(df,train=True,game=GAME,seed=SEED):\n    if train:\n        filename= 'train_scores'\n    else:\n        filename= 'test_scores'\n    results_folder = \"results\"\n    if not os.path.exists(results_folder):\n        os.makedirs(results_folder)\n    path = f\"{results_folder}/{game}_{filename}_{SEED}.csv\"\n    df.to_csv(path)\n    print(f\"file created in {path}\")\n          \ndef load_data(game,seed,train=True):\n    if train:\n        filename= 'train_scores'\n    else:\n        filename= 'test_scores'\n    results_folder = \"results\"\n    path = f\"{results_folder}/{game}_{filename}_{SEED}.csv\"\n    df = pd.read_csv(path)\n    print(f\"file csv read from {path}\")\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:27:45.582996Z","iopub.execute_input":"2024-10-14T18:27:45.583586Z","iopub.status.idle":"2024-10-14T18:27:45.590411Z","shell.execute_reply.started":"2024-10-14T18:27:45.583555Z","shell.execute_reply":"2024-10-14T18:27:45.589499Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"test_seeds = list(np.random.randint(low = 2000,high=5000, size = 50))\n#evaluate the agent on the full distribution of levels\ndef fixed_set_evaluation(agent,test_seeds):\n    scores = []\n    for seed in test_seeds:\n        sum = 0\n        done = False\n        truncated = False\n        test_env = gym.make(f'procgen:procgen-{game}-v0',\n               #render='human', \n               distribution_mode = 'easy', \n               #use_backgrounds=False,\n               apply_api_compatibility = True,\n               start_level = int(seed) ,\n               rand_seed = int(seed)\n               )\n        state,_= test_env.reset()\n        agent.actor.eval()\n        with torch.no_grad():\n            while not(done or truncated):\n                prob, action=agent.get_action(state)\n                act = int(action.detach().cpu().numpy())\n                next_state, reward, done, truncated, info = test_env.step(act)\n                state = next_state\n                sum = sum + reward\n            scores.append(sum)\n    average_score = np.mean(scores)\n    agent.actor.train()\n    return average_score\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-14T18:27:46.221200Z","iopub.execute_input":"2024-10-14T18:27:46.221531Z","iopub.status.idle":"2024-10-14T18:27:46.229950Z","shell.execute_reply.started":"2024-10-14T18:27:46.221498Z","shell.execute_reply":"2024-10-14T18:27:46.229122Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#evaluation on the full distribution\ndef evaluate(agent,test_env,test_size=10):\n    scores = []\n    lengths = []\n    for i in range(test_size):\n        sum = 0\n        l = 0\n        done = False\n        truncated = False\n        state,_= test_env.reset()\n        #pass the model to evaluation mode, to deal with batch_norm and dropout layers\n        agent.actor.eval()\n        with torch.no_grad():\n            while not(done or truncated):\n                _, action = agent.get_action(state)\n                act = int(action.detach().cpu().numpy())\n                next_state, reward, done, truncated, info = test_env.step(act)\n                state = next_state\n                l = l + 1\n                sum = sum + reward\n            scores.append(sum)\n            lengths.append(l)\n    average_score = np.mean(scores)\n    average_length = np.mean(lengths)\n    #return back to training mode\n    agent.actor.train()\n    return average_score,average_length\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:27:46.614313Z","iopub.execute_input":"2024-10-14T18:27:46.614982Z","iopub.status.idle":"2024-10-14T18:27:46.622415Z","shell.execute_reply.started":"2024-10-14T18:27:46.614955Z","shell.execute_reply":"2024-10-14T18:27:46.621425Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gym Environments","metadata":{}},{"cell_type":"code","source":"game = GAME\nn_envs = 48\ntest_size = 10\n#train on a fixed subset of levels in easy mode\nenv = gym.vector.make(f'procgen:procgen-{game}-v0',\n               #render='human', \n               num_levels = 200, \n               distribution_mode = 'easy', \n               #use_backgrounds=False,\n               apply_api_compatibility = True,\n               start_level = SEED,\n               rand_seed = SEED,\n               num_envs=n_envs\n               )\nprint(f\"observation space shape:{env.observation_space.shape}\")\nprint(f\"action space size: {env.action_space}\")\n\nn_states = env.observation_space.shape[0]\nn_actions = np.array(env.action_space)[0]\n\n\n#evaluate the agent on the full distribution of levels\ntest_env = gym.make(f'procgen:procgen-{game}-v0',\n                   #render='human', \n                    distribution_mode = 'easy',\n                    #num_levels = 0,\n                    #use_backgrounds=False,\n                    apply_api_compatibility = True,\n                    start_level = SEED,\n                    rand_seed = SEED)\n#use this env to evaluate the agent on the trained levels\ntrain_env = gym.make(f'procgen:procgen-{game}-v0',\n                   #render='human', \n                    distribution_mode = 'easy',\n                    num_levels = 200,\n                    #use_backgrounds=False,\n                    apply_api_compatibility = True,\n                    start_level = SEED,\n                    rand_seed = SEED)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:27:47.580641Z","iopub.execute_input":"2024-10-14T18:27:47.580946Z","iopub.status.idle":"2024-10-14T18:27:50.171072Z","shell.execute_reply.started":"2024-10-14T18:27:47.580923Z","shell.execute_reply":"2024-10-14T18:27:50.169805Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Execution","metadata":{}},{"cell_type":"code","source":"def exec(agent,\n         env,\n         test_env,\n         iterations,\n         batch_size,\n         n_envs,\n         results,\n         device,\n         seed=SEED):\n    \n    best_model = {'model':agent.actor.state_dict(),\n                  'score':0,\n                  'iteration':0}\n    #save results at each checkpoint\n    results_folder = \"results\"\n    filename = 'all_res'\n    if not os.path.exists(results_folder):\n        os.makedirs(results_folder)\n    path = f\"{results_folder}/{GAME}_{filename}_{SEED}.pkl\"\n\n    #every 100 eps terminated compute an average and save the mean over 100 eps, and the start collecting again \n    train_scores = []\n    train_steps = []\n    train_lengths = []\n    test_scores = []\n    test_steps = []\n    test_lengths = []\n    ftr_scores = [] #fixed train env\n    ftr_lengths = []\n    #count the total step\n    tot_steps = 0\n    terminated_ep_rewards = [] #contains all the rewards from all the environments of all the terminated episodes\n    #reset the environments\n    states,_ = env.reset()\n    states=torch.stack([torch.tensor(o,device=device) for o in states])\n    #buffer used to compute the average reward of the last 100 terminated episodes \n    temp_scores = []\n    temp_lengths = []\n    actor_loss = []\n    critic_loss = []\n    for it in tqdm(range(1,iterations+1)):\n        #inearly anneal the learning rates at each iteration using scale\n        scale = 1 - ((it-1)/1500)\n        # anneal the optimizer's learning rate\n        agent.actor_optimizer.param_groups[0][\"lr\"] = agent.lr_a * scale\n        agent.critic_optimizer.param_groups[0][\"lr\"] = agent.lr_c * scale\n        \n        #used to collect informations from the environments\n        batch_states = []\n        batch_values = []\n        batch_rewards =  []\n        batch_dones = []\n        batch_actions = []\n        batch_log_probs = []\n        sum_rewards = np.zeros((n_envs,), dtype=float)\n        sum_lengths = np.zeros((n_envs,),dtype=int)\n        \n        for t in range(batch_size):\n            #play a single step in the environments\n            next_states, rewards,dones,log_probs,actions = agent.play_step(states)\n            #add the new rewards to the previous ones\n            sum_rewards = sum_rewards + rewards\n            sum_lengths = sum_lengths + np.ones((n_envs,),dtype=int)\n            #print(sum_rewards)\n            for i in range(len(dones)):\n                if dones[i]:\n                    terminated_ep_rewards.append(sum_rewards[i])\n                    temp_scores.append(sum_rewards[i])\n                    temp_lengths.append(sum_lengths[i])\n            #compute an average score of the last 100 terminated episodes\n            if len(temp_scores)>=100:\n                train_scores.append(np.mean(temp_scores[-100:]))\n                #save also the total num of steps when average is computed \n                train_steps.append(tot_steps)\n                train_lengths.append(np.mean(temp_lengths[-100:]))\n                #reset the buffer for new collection \n                temp_scores = []\n                temp_lengths = []\n            #reset tot rewards for the terminated episodes\n            sum_rewards[dones] = 0\n            #reset tot length for the terminated episodes\n            sum_lengths[dones] = 0\n            #compute state values using the critic\n            values = agent.critic(states).squeeze().to(device).detach()\n            #update the batches \n            batch_values.append(values)\n            batch_rewards.append(rewards)\n            batch_states.append(states)\n            batch_dones.append(dones)\n            batch_actions.append(actions)\n            batch_log_probs.append(log_probs)\n            states = torch.stack([torch.tensor(o,device=device) for o in next_states])\n            #increase the tot_step\n            tot_steps += 1\n        #compute the next_values of the last states,to be used for the computation of the advantages\n        next_values = agent.critic(states).squeeze().to(device).detach()\n        batch_values = torch.stack(batch_values).to(device)\n        batch_dones = torch.stack([torch.tensor(d, device=device, dtype=torch.bool) for d in batch_dones]).to(device)\n        #batch_rewards = torch.stack(batch_rewards).to(device)\n        batch_rewards = torch.stack([torch.tensor(r, device=device) for r in batch_rewards]).to(device)\n        #compute GAE advantages and returns (will be used as critic targets) for each timestep\n        advs,returns = agent.compute_advs_rts(batch_values,batch_rewards,batch_dones,next_values)\n\n        #flatten the data collected from the different environments\n        batch_states = torch.stack(batch_states)\n        #reshape the batches by collapsing the batch and n_environment dimensions\n        obs_size = batch_states.size()[2:]\n        batch_states = batch_states.reshape((tuple([n_envs*batch_size])+obs_size))\n        advs = torch.stack(advs).reshape(-1,1).to(device)\n        returns = torch.stack(returns).reshape(-1,1).to(device)\n        batch_actions = torch.stack(batch_actions).reshape(-1).to(device).detach()\n        batch_log_probs = torch.stack(batch_log_probs).reshape((-1,1)).to(device).detach()\n\n        #learning phase\n        a_loss, c_loss = agent.train(batch_states,advs,returns,batch_log_probs,batch_actions)\n        actor_loss.append(a_loss)\n        critic_loss.append(c_loss)\n        print(f\"mean_reward: {np.mean(terminated_ep_rewards[-20:])}, n°of steps: {tot_steps}, n° of terminated episodes: {len(terminated_ep_rewards)}\")\n        #evaluation phase, after each iteration compute an evaluation score\n        if ((it%2) == 0):\n            test_score,test_l = evaluate(agent,test_env)\n            ftr_score,ftr_l = evaluate(agent,train_env)\n            ftr_scores.append(ftr_score)\n            ftr_lengths.append(ftr_l)\n            test_scores.append(test_score)\n            test_steps.append(tot_steps)\n            test_lengths.append(test_l)\n            if(best_model['score']<=(np.mean(test_scores[-2:]))):\n                #save the model parameters\n                best_model.update({'model':agent.actor.state_dict(),'score':test_score,'iteration':it})\n                \n            print(f\"iteration:{it},'tot_steps': {tot_steps}, test_score:{test_score}, train_score:{ftr_score}\")\n            #if (eval_score >= 8.5):\n                #break\n        if (it%250 == 0):\n            print(f\"iteration:{it}, tot_steps:{tot_steps}\")\n            #save checkpoint\n            agent.save_checkpoint(step=it)\n            #save best_model\n            save_model_weights(best_model['model'],seed=seed,best=True)\n        if (it%100 == 0):\n            #save results to pkl file\n            with open(path, 'wb') as f:\n                pickle.dump(final_res, f)\n        if (it%10 == 0):    \n            final_res = {'train_scores':train_scores,'train_steps':train_steps,'train_lengths':train_lengths,\n                         'test_scores':test_scores,'test_steps':test_steps,'test_lengths':test_lengths,\n                         'tot_eps_rws':terminated_ep_rewards,'best_model':best_model,'actor_loss':actor_loss,\n                         'critic_loss':critic_loss,'ftr_scores': ftr_scores,'ftr_lengths':ftr_lengths}\n\n            results.update(final_res)\n                            \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:27:50.173995Z","iopub.execute_input":"2024-10-14T18:27:50.174320Z","iopub.status.idle":"2024-10-14T18:27:50.205558Z","shell.execute_reply.started":"2024-10-14T18:27:50.174294Z","shell.execute_reply":"2024-10-14T18:27:50.204696Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#tot_timesteps = 1000 * 256 * 48 = 12 288 000 \n#parameters\nin_ch = 3\nn_actions = 15\nbatch_size = 256\niterations = 750\nthreshold = 30\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nagent = PPOAgent(in_ch,n_actions,n_envs=n_envs,batch_size=batch_size,device=device )","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:27:50.206679Z","iopub.execute_input":"2024-10-14T18:27:50.206997Z","iopub.status.idle":"2024-10-14T18:27:51.636869Z","shell.execute_reply.started":"2024-10-14T18:27:50.206967Z","shell.execute_reply":"2024-10-14T18:27:51.636098Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#used for debugging\nresults = {}\nfinal_results = exec(agent,\n     env,\n     test_env,\n     iterations,\n     batch_size,\n     n_envs,\n     results,\n     device,\n     SEED)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-14T18:27:51.638865Z","iopub.execute_input":"2024-10-14T18:27:51.639360Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate(agent, test_env,100)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:49:26.455369Z","iopub.execute_input":"2024-09-21T15:49:26.455739Z","iopub.status.idle":"2024-09-21T15:51:45.152247Z","shell.execute_reply.started":"2024-09-21T15:49:26.455708Z","shell.execute_reply":"2024-09-21T15:51:45.151295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(f'results/{GAME}_all_res_{SEED}.pkl', 'rb') as f:\n    results = pickle.load(f)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:08:42.057901Z","iopub.execute_input":"2024-10-05T09:08:42.058591Z","iopub.status.idle":"2024-10-05T09:08:42.089437Z","shell.execute_reply.started":"2024-10-05T09:08:42.058546Z","shell.execute_reply":"2024-10-05T09:08:42.088634Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results.keys()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:08:44.416942Z","iopub.execute_input":"2024-10-05T09:08:44.417673Z","iopub.status.idle":"2024-10-05T09:08:44.425749Z","shell.execute_reply.started":"2024-10-05T09:08:44.417631Z","shell.execute_reply":"2024-10-05T09:08:44.424681Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_model_weights(agent.actor.state_dict(),best=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:48:28.935672Z","iopub.execute_input":"2024-09-21T15:48:28.936449Z","iopub.status.idle":"2024-09-21T15:48:28.956954Z","shell.execute_reply.started":"2024-09-21T15:48:28.936417Z","shell.execute_reply":"2024-09-21T15:48:28.955880Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sc,st,l = results['train_scores'],results['train_steps'],results['train_lengths']\ndata = {'scores': sc, 'steps': st, 'lengths':l}\ndf_train = pd.DataFrame(data)\ndf_train['steps'] = df_train['steps']*n_envs\ndf_train","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:08:49.126551Z","iopub.execute_input":"2024-10-05T09:08:49.127630Z","iopub.status.idle":"2024-10-05T09:08:49.158778Z","shell.execute_reply.started":"2024-10-05T09:08:49.127584Z","shell.execute_reply":"2024-10-05T09:08:49.157825Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_scores,test_steps = results['test_scores'],results['test_steps']\ndata1 = {'scores': test_scores, 'steps': test_steps}\ndf_test = pd.DataFrame(data1)\ndf_test['steps']=df_test['steps']*n_envs\ndf_test","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:08:50.366083Z","iopub.execute_input":"2024-10-05T09:08:50.366439Z","iopub.status.idle":"2024-10-05T09:08:50.377637Z","shell.execute_reply.started":"2024-10-05T09:08:50.366406Z","shell.execute_reply":"2024-10-05T09:08:50.376695Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ftr_scores,ftr_steps = results['ftr_scores'],results['test_steps']\ndata2 = {'scores': ftr_scores, 'steps': ftr_steps}\ndf_ftr = pd.DataFrame(data2)\ndf_ftr['steps']=df_ftr['steps']*n_envs\ndf_ftr","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:08:57.905842Z","iopub.execute_input":"2024-10-05T09:08:57.906227Z","iopub.status.idle":"2024-10-05T09:08:57.917431Z","shell.execute_reply.started":"2024-10-05T09:08:57.906189Z","shell.execute_reply":"2024-10-05T09:08:57.916530Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#agent.actor.load_state_dict(results['best_model']['model'])","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:51:58.371523Z","iopub.execute_input":"2024-09-21T15:51:58.372379Z","iopub.status.idle":"2024-09-21T15:51:58.382962Z","shell.execute_reply.started":"2024-09-21T15:51:58.372344Z","shell.execute_reply":"2024-09-21T15:51:58.381928Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#evaluate(agent,test_env,500)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:51:58.723482Z","iopub.execute_input":"2024-09-21T15:51:58.724171Z","iopub.status.idle":"2024-09-21T15:54:11.538623Z","shell.execute_reply.started":"2024-09-21T15:51:58.724143Z","shell.execute_reply":"2024-09-21T15:54:11.537528Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x=df_train['scores'].rolling(10).mean()\nftr = df_ftr['scores'].rolling(20).mean()\nt = df_test['scores'].rolling(20).mean()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:09:31.845542Z","iopub.execute_input":"2024-10-05T09:09:31.845949Z","iopub.status.idle":"2024-10-05T09:09:31.851462Z","shell.execute_reply.started":"2024-10-05T09:09:31.845908Z","shell.execute_reply":"2024-10-05T09:09:31.850206Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(df_ftr['steps']/1e6,ftr,label='train')\nplt.plot(df_test['steps']/1e6,t,label='test')\nplt.xlabel('steps')\nplt.ylabel('mean rewards')\n# plotting the legend \nplt.legend(loc = 'lower right') \nplt.savefig(f'{GAME}_{SEED}.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:09:58.419999Z","iopub.execute_input":"2024-10-05T09:09:58.420396Z","iopub.status.idle":"2024-10-05T09:09:58.740364Z","shell.execute_reply.started":"2024-10-05T09:09:58.420355Z","shell.execute_reply":"2024-10-05T09:09:58.739453Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df = pd.DataFrame(results['tot_eps_rws'])\n#plt.plot(df.rolling(50).mean())\nplt.plot(df_train['steps']/1e6,x,label='train')\nplt.xlabel('steps')\nplt.ylabel('mean rewards')\nplt.savefig(f'avgscores_{GAME}_{SEED}.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:11:54.152929Z","iopub.execute_input":"2024-10-05T09:11:54.153338Z","iopub.status.idle":"2024-10-05T09:11:54.456242Z","shell.execute_reply.started":"2024-10-05T09:11:54.153299Z","shell.execute_reply":"2024-10-05T09:11:54.455338Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(df_train['steps'],x)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T22:55:16.943097Z","iopub.execute_input":"2024-09-20T22:55:16.944091Z","iopub.status.idle":"2024-09-20T22:55:17.172183Z","shell.execute_reply.started":"2024-09-20T22:55:16.944053Z","shell.execute_reply":"2024-09-20T22:55:17.171335Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(df_test['steps'],t)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T22:55:09.261462Z","iopub.execute_input":"2024-09-20T22:55:09.262431Z","iopub.status.idle":"2024-09-20T22:55:09.500102Z","shell.execute_reply.started":"2024-09-20T22:55:09.262394Z","shell.execute_reply":"2024-09-20T22:55:09.499206Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_training(score,steps,threshold):\n    #each 50 episode take the average score over the 50 eps passed\n    avg_score = []\n    eps = []\n    for i in range(1, len(score)+1):\n        #if (i % 50 == 0):\n        #smooth_scores\n        avg_score.append(np.mean(score[i:i+300]))\n            #avg_score.append(score[i])\n        eps.append(i)\n    #print(len(avg_score))\n    label = 'Procgen_PPO'\n    #plt.plot(eps,avg_score,'b.')\n    plt.plot(avg_score,'b', label = label)\n    plt.axhline(y = threshold, color = 'r', linestyle = '--') \n    plt.xlabel('steps')\n    plt.ylabel('average rewards')\n    # plotting the legend \n    plt.legend(bbox_to_anchor = (1.1, 1.1), loc = 'upper right') \n    plt.show()\n#plot_training(terminated_ep_rewards,steps,10)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T23:10:31.312767Z","iopub.execute_input":"2024-09-19T23:10:31.313549Z","iopub.status.idle":"2024-09-19T23:10:33.299000Z","shell.execute_reply.started":"2024-09-19T23:10:31.313500Z","shell.execute_reply":"2024-09-19T23:10:33.298110Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training(score,threshold):\n    #each 50 episode take the average score over the 50 eps passed\n    avg_score = []\n    eps = []\n    for i in range(1, len(score)+1):\n        if (i % 30 == 0):\n            avg_score.append(np.mean(score[i-30:i]))\n            #avg_score.append(score[i])\n            eps.append(i)\n    label = 'n-stepA2C'\n    plt.plot(eps,avg_score,'g^')\n    plt.plot(eps,avg_score,'g', label = label)\n    plt.axhline(y = threshold, color = 'r', linestyle = '--') \n    plt.xlabel('episodes')\n    plt.ylabel('average rewards each 20 eps')\n    # plotting the legend \n    plt.legend(bbox_to_anchor = (1.1, 1.1), loc = 'upper right') \n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2024-09-05T16:47:48.260855Z","iopub.status.busy":"2024-09-05T16:47:48.259952Z","iopub.status.idle":"2024-09-05T16:47:48.267917Z","shell.execute_reply":"2024-09-05T16:47:48.266891Z","shell.execute_reply.started":"2024-09-05T16:47:48.260819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n#COINRUN SEED 42\n#used for debugging\nresults = {}\nfinal_results = exec(agent,\n     env,\n     test_env,\n     iterations,\n     batch_size,\n     n_envs,\n     results,\n     device,\n     SEED)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-09-20T16:39:50.568472Z","iopub.execute_input":"2024-09-20T16:39:50.569227Z","iopub.status.idle":"2024-09-20T22:22:25.300682Z","shell.execute_reply.started":"2024-09-20T16:39:50.569194Z","shell.execute_reply":"2024-09-20T22:22:25.299283Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null}]}
